#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

FROM openjdk:8-alpine

# Install gcloud to grab reference genome and known indels
ENV CLOUD_SDK_VERSION 206.0.0

ENV PATH /google-cloud-sdk/bin:$PATH
RUN apk --no-cache add \
        curl \
        python \
        py-crcmod \
        bash \
        libc6-compat \
        openssh-client \
        git \
    && curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-${CLOUD_SDK_VERSION}-linux-x86_64.tar.gz && \
    tar xzf google-cloud-sdk-${CLOUD_SDK_VERSION}-linux-x86_64.tar.gz && \
    rm google-cloud-sdk-${CLOUD_SDK_VERSION}-linux-x86_64.tar.gz && \
    ln -s /lib /lib64 && \
    gcloud config set core/disable_usage_reporting true && \
    gcloud config set component_manager/disable_update_check true && \
    gcloud config set metrics/environment github_docker_image && \
    gcloud --version
VOLUME ["/root/.config"]


ARG BASE_DIRECTORY=spark-kubernetes
ARG SPARK_DISTRIBUTION
ARG SPARK_PATH=${BASE_DIRECTORY}/${SPARK_DISTRIBUTION}
ARG JAR_FILE

# Before building the docker image, first build and make a Spark distribution following
# the instructions in http://spark.apache.org/docs/latest/building-spark.html.
# If this docker file is being used in the context of building your images from a Spark
# distribution, the docker build command should be invoked from the top level directory
# of the Spark distribution. E.g.:
# docker build -t spark:latest -f kubernetes/dockerfiles/spark/Dockerfile_standalone .

RUN set -ex && \
    apk upgrade --no-cache && \
    apk add --no-cache bash tini libc6-compat build-base zlib-dev syslinux-dev wget git && \
    mkdir -p /opt/spark && \
    mkdir -p /opt/spark/work-dir \
    touch /opt/spark/RELEASE && \
    rm /bin/sh && \
    ln -sv /bin/bash /bin/sh && \
    chgrp root /etc/passwd && chmod ug+rw /etc/passwd

WORKDIR /tmp
RUN git clone https://github.com/lh3/bwa.git
WORKDIR /tmp/bwa
RUN git checkout v0.7.17
RUN make
RUN cp -p bwa /usr/local/bin
RUN rm -rf /tmp/bwa
WORKDIR /

ADD ${SPARK_PATH}/jars /opt/spark/jars
ADD ${SPARK_PATH}/bin /opt/spark/bin
ADD ${SPARK_PATH}/sbin /opt/spark/sbin
ADD ${SPARK_PATH}/conf /opt/spark/conf
ADD ${SPARK_PATH}/entrypoint.sh /opt/
ADD ${SPARK_PATH}/data /opt/spark/data
ADD ${BASE_DIRECTORY}/conf/pipeline.yaml /opt/spark/work-dir/conf/pipeline.yaml
ADD target/lib /opt/spark/jars
ADD target/${JAR_FILE} /usr/share/pipeline5/system.jar

ENV SPARK_HOME /opt/spark

WORKDIR /opt/spark/work-dir

ENTRYPOINT [ "/opt/entrypoint.sh" ]
